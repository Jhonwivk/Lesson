# hw2

## 权重初始化

**Xavier uniform**

![Untitled](hw2%20de0b05f6c3e44c93844877c49601fb23/Untitled.png)

**Kaiming normal**

![Untitled](hw2%20de0b05f6c3e44c93844877c49601fb23/Untitled%201.png)

![Untitled](hw2%20de0b05f6c3e44c93844877c49601fb23/Untitled%202.png)

for relu  gian=$\sqrt 2$

![Untitled](hw2%20de0b05f6c3e44c93844877c49601fb23/Untitled%203.png)

```python
def xavier_uniform(fan_in, fan_out, gain=1.0, **kwargs):
    ### BEGIN YOUR SOLUTION
    a = gain * math.sqrt(6/(fan_in+fan_out))
    return a*(2*rand(fan_in,fan_out,**kwargs)-1)
    ### END YOUR SOLUTION

def xavier_normal(fan_in, fan_out, gain=1.0, **kwargs):
    ### BEGIN YOUR SOLUTION
    std = gain * math.sqrt(2/(fan_in+fan_out))
    return  std*randn(fan_in,fan_out,**kwargs)
    ### END YOUR SOLUTION

def kaiming_uniform(fan_in, fan_out, nonlinearity="relu", **kwargs):
    assert nonlinearity == "relu", "Only relu supported currently"
    ### BEGIN YOUR SOLUTION
    gain = math.sqrt(2)
    bound = gain * math.sqrt(3/fan_in)
    return bound*(2*rand(fan_in,fan_out,**kwargs)-1)
    ### END YOUR SOLUTION

def kaiming_normal(fan_in, fan_out, nonlinearity="relu", **kwargs):
    assert nonlinearity == "relu", "Only relu supported currently"
    ### BEGIN YOUR SOLUTION
    gain = math.sqrt(2)
    std = gain / math.sqrt(fan_in)
    return  std*randn(fan_in,fan_out,**kwargs)
    ### END YOUR SOLUTION
```

## modue 模块

```python
class Module:
    def __init__(self):
        self.training = True
		# 递归解析参数  
    def parameters(self) -> List[Tensor]:
        """Return the list of parameters in the module."""
        return _unpack_params(self.__dict__)
		#子类 递归解析
    def _children(self) -> List["Module"]:
        return _child_modules(self.__dict__)

    def eval(self):
        self.training = False
        for m in self._children():
            m.training = False

    def train(self):
        self.training = True
        for m in self._children():
            m.training = True

    def __call__(self, *args, **kwargs):
        return self.forward(*args, **kwargs)
```

```python
class Linear(Module):
    def __init__(self, in_features, out_features, bias=True, device=None, dtype="float32"):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(init.kaiming_uniform(in_features,out_features,requires_grad=True))
        if bias:
            self.bias = Parameter(init.kaiming_uniform(out_features, 1, requires_grad=True).reshape((1, out_features)))
        else:
            self.bias = None
        ### BEGIN YOUR SOLUTION
        ### END YOUR SOLUTION

    def forward(self, X: Tensor) -> Tensor:
        ### BEGIN YOUR SOLUTION
        xA = X @ self.weight
        if self.bias:
          y=xA + self.bias.broadcast_to(xA.shape)
        else:
          y=xA
        return y 
        ### END YOUR SOLUTION
```

layer→forward的过程 实际上TensorOp的构建计算图的过程              以Linear为例

```python
y=xA + self.bias.broadcast_to(xA.shape)
```

涉及到 add，broadcast_两个算子，数据流在算子间传递

动态图:  边构建 边计算

![Untitled](hw2%20de0b05f6c3e44c93844877c49601fb23/Untitled%204.png)

- BN是在batch上，对N、H、W做归一化，而保留通道 C 的维度。BN对较小的batch size效果不好。BN适用于固定深度的前向神经网络，如CNN，不适用于RNN；
LN在通道方向上，对C、H、W归一化，主要对RNN效果明显，Transformer上的归一化也是用的LN；
IN在图像像素上，对H、W做归一化，用在风格化迁移；
GN将channel分组，然后再做归一化。

BatchNorm后是不改变输入的shape的；

```
    nn.BatchNorm1d： N * d --> N * d

    nn.BatchNorm2d: N * C * H * W  -- > N * C * H * W

    nn.BatchNorm3d: N * C * d * H * W --> N * C * d * H * W

```

![Untitled](hw2%20de0b05f6c3e44c93844877c49601fb23/Untitled%205.png)

```python
class BatchNorm1d(Module):
    def __init__(self, dim, eps=1e-5, momentum=0.1, device=None, dtype="float32"):
        super().__init__()
        self.dim = dim
        self.eps = eps
        self.momentum = momentum
        ### BEGIN YOUR SOLUTION
        self.weight = Parameter(init.ones(self.dim, requires_grad=True))
        self.bias = Parameter(init.zeros(self.dim, requires_grad=True))
        self.running_mean = init.zeros(self.dim)
        self.running_var = init.ones(self.dim)
        ### END YOUR SOLUTION

    def forward(self, x: Tensor) -> Tensor:
        ### BEGIN YOUR SOLUTION
        batch_size = x.shape[0]
        mean = x.sum((0, )) / batch_size
        # NOTE array with shape (4, ) is considered as a row, so it can be brcsto (2, 4) and cannot be brcsto (4, 2)
        x_minus_mean = x - mean.broadcast_to(x.shape)
        var = (x_minus_mean ** 2).sum((0, )) / batch_size
        ### 推理时用running_mean running_var
        if self.training:
            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data
            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.data

            x_std = ((var + self.eps) ** 0.5).broadcast_to(x.shape)
            x_normed = x_minus_mean / x_std
            return x_normed * self.weight.broadcast_to(x.shape) + self.bias.broadcast_to(x.shape)
        else:  
            # NOTE no momentum here!
            x_normed = (x - self.running_mean) / (self.running_var + self.eps) ** 0.5
            # NOTE testing time also need self.weight and self.bias
            return x_normed * self.weight.broadcast_to(x.shape) + self.bias.broadcast_to(x.shape)
        ### END YOUR SOLUTION
```

- Flattren
- LayerNorm1d
- Dropout
- Residual  :传一个module  输出 x+module(x)

实现比较naive

## Optim

> 优化时的时候 一定要，param更新需要detach，否则会构建新的节点
> 

```python
class SGD(Optimizer):
    def __init__(self, params, lr=0.01, momentum=0.0, weight_decay=0.0):
        super().__init__(params)
        self.lr = lr
        self.momentum = momentum
        self.u = {}
        self.weight_decay = weight_decay

    def step(self): #opt时,grad更新需要detach,不然会创建新的变量。
    #除非是计算图构建，其余时候计算，用x.data就行
        for i,param in enumerate(self.params):
          if i not in self.u:
            self.u[i]=0
          if param.grad is None:
              continue
          self.u[i]=self.u[i]*self.momentum+(1-self.momentum)*(ndl.Tensor(param.grad.numpy(), dtype='float32').data+self.weight_decay*param.data)
          param.data = param.data - self.lr * self.u[i] 

class Adam(Optimizer):
    def __init__(
        self,
        params,
        lr=0.01,
        beta1=0.9,
        beta2=0.999,
        eps=1e-8,
        weight_decay=0.0,
    ):
        super().__init__(params)
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.weight_decay = weight_decay
        self.t = 0

        self.m = {}
        self.v = {}

    def step(self):
        ### BEGIN YOUR SOLUTION
        self.t +=1
        for i,param in enumerate(self.params):
          if i not in self.m:
            self.m[i] = 0
            self.v[i] = 0
          if param.grad is None:
            continue
          grad =  (ndl.Tensor(param.grad.numpy(), dtype='float32').data+self.weight_decay*param.data)
          self.m[i] = self.beta1*self.m[i]+(1-self.beta1)*grad
          self.v[i] = self.beta2*self.v[i]+(1-self.beta2)*grad**2
          u_hat = self.m[i]/(1-self.beta1**self.t)
          v_hat = self.v[i]/(1-self.beta2**self.t)
          param.data = param.data - self.lr*u_hat/(v_hat**0.5+self.eps)

        ### END YOUR SOLUTION
```

SGD

![Untitled](hw2%20de0b05f6c3e44c93844877c49601fb23/Untitled%206.png)

Adam

![Untitled](hw2%20de0b05f6c3e44c93844877c49601fb23/Untitled%207.png)