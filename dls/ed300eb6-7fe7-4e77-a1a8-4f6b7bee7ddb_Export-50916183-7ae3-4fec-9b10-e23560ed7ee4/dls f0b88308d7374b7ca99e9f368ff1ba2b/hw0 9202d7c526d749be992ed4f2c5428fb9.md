# hw0

### softmax

```python
def softmax_loss(Z, y):
    """ Return softmax loss.  Note that for the purposes of this assignment,
    you don't need to worry about "nicely" scaling the numerical properties
    of the log-sum-exp computation, but can just compute this directly.

    Args:
        Z (np.ndarray[np.float32]): 2D numpy array of shape
            (batch_size, num_classes), containing the logit predictions for
            each class.
        y (np.ndarray[np.int8]): 1D numpy array of shape (batch_size, )
            containing the true label of each example.

    Returns:
        Average softmax loss over the sample.
    """
    ### BEGIN YOUR CODE
    Z_y=Z[np.arange(Z.shape[0]),y]
    Z = np.log(np.exp(Z).sum(axis=1))
    return np.mean(Z - Z_y)
    ### END YOUR CODE
```

![Untitled](hw0%209202d7c526d749be992ed4f2c5428fb9/Untitled.png)

### softmax_regression_epoch

```python
def softmax_regression_epoch(X, y, theta, lr = 0.1, batch=100):
    """ Run a single epoch of SGD for softmax regression on the data, using
    the step size lr and specified batch size.  This function should modify the
    theta matrix in place, and you should iterate through batches in X _without_
    randomizing the order.

    Args:
        X (np.ndarray[np.float32]): 2D input array of size
            (num_examples x input_dim).
        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)
        theta (np.ndarrray[np.float32]): 2D array of softmax regression
            parameters, of shape (input_dim, num_classes)
        lr (float): step size (learning rate) for SGD
        batch (int): size of SGD minibatch

    Returns:
        None
    """
    ### BEGIN YOUR CODE
    num = X.shape[0]
    for i in range(num//batch+1):
      if (i == num//batch)&(num%batch==0):
        break
      m = batch if(i!=num//batch) else num%batch
      X_b = X[i*batch:min((i+1)*batch,num)]
      y_b = y[i*batch:min((i+1)*batch,num)]
      temp = np.exp(X_b @ theta)
      Z = temp  / ((temp.sum(axis=1)))[:,None]
      I = np.zeros_like(Z)
      I[np.arange(Z.shape[0]),y_b] = 1
      grad = (1/m) * np.transpose(X_b) @ (Z - I)
      theta -= lr * grad 

    ### END YOUR CODE
```

![Untitled](hw0%209202d7c526d749be992ed4f2c5428fb9/Untitled%201.png)

### two FC

```python
def nn_epoch(X, y, W1, W2, lr = 0.1, batch=100):
    """ Run a single epoch of SGD for a two-layer neural network defined by the
    weights W1 and W2 (with no bias terms):
        logits = ReLU(X * W1) * W2
    The function should use the step size lr, and the specified batch size (and
    again, without randomizing the order of X).  It should modify the
    W1 and W2 matrices in place.

    Args:
        X (np.ndarray[np.float32]): 2D input array of size
            (num_examples x input_dim).
        y (np.ndarray[np.uint8]): 1D class label array of size (num_examples,)
        W1 (np.ndarray[np.float32]): 2D array of first layer weights, of shape
            (input_dim, hidden_dim)
        W2 (np.ndarray[np.float32]): 2D array of second layer weights, of shape
            (hidden_dim, num_classes)
        lr (float): step size (learning rate) for SGD
        batch (int): size of SGD minibatch

    Returns:
        None
    """
    ### BEGIN YOUR CODE
    absrelu = lambda x:(abs(x)+x)/2
    num = X.shape[0]
    for i in range(num//batch+1):
      start,end = i*batch,min((i+1)*batch,num)
      m = end - start 
      if m==0:
        break
      X_b , y_b = X[start:end] , y[start:end]
      Z_1 = absrelu(X_b @ W1)
      temp = np.exp(Z_1 @ W2)
      noml_Z = temp / (temp.sum(axis=1)).reshape(-1,1)
      I_y = np.zeros_like(noml_Z)
      I_y[np.arange(I_y.shape[0]),y_b] = 1
      G_2 = noml_Z - I_y
      I_p = np.zeros_like(Z_1)
      I_p[Z_1>0] = 1
      G_1 = I_p * (G_2 @ W2.T)
      grad_W1 = (1/m) * (X_b.T @ G_1)
      grad_W2 = (1/m) * (Z_1.T @ G_2)
      W1 -= lr * grad_W1
      W2 -= lr * grad_W2
    ### END YOUR CODE

### CODE BELOW IS FOR ILLUSTRATION, YOU DO NOT NEED TO EDIT

def loss_err(h,y):
    """ Helper funciton to compute both loss and error"""
    return softmax_loss(h,y), np.mean(h.argmax(axis=1) != y)

def train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr=0.5, batch=100,
                  cpp=False):
    """ Example function to fully train a softmax regression classifier """
    theta = np.zeros((X_tr.shape[1], y_tr.max()+1), dtype=np.float32)
    print("| Epoch | Train Loss | Train Err | Test Loss | Test Err |")
    for epoch in range(epochs):
        if not cpp:
            softmax_regression_epoch(X_tr, y_tr, theta, lr=lr, batch=batch)
        else:
            softmax_regression_epoch_cpp(X_tr, y_tr, theta, lr=lr, batch=batch)
        train_loss, train_err = loss_err(X_tr @ theta, y_tr)
        test_loss, test_err = loss_err(X_te @ theta, y_te)
        print("|  {:>4} |    {:.5f} |   {:.5f} |   {:.5f} |  {:.5f} |"\
              .format(epoch, train_loss, train_err, test_loss, test_err))
```